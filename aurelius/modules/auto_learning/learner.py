"""
AURELIUS Auto-Learning Module
Continuously learns from interactions, sales data, and engagement patterns to optimize strategies.
"""

import asyncio
import json
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
import re

from ...config import config
from ...logging_config import get_logger, log_data_operation
from ...db.redis_client import data_client
from ...core.ai import ai_service

logger = get_logger("AUTO_LEARNING")

class AureliusLearner:
    """
    Auto-learning system that analyzes patterns in content performance, engagement, and sales
    to continuously improve content strategies and business outcomes.
    """
    
    def __init__(self):
        # Learning data storage keys
        self.patterns_key = "learning:patterns"
        self.insights_key = "learning:insights"
        self.strategies_key = "learning:strategies"
        self.performance_key = "learning:performance"
        self.recommendations_key = "learning:recommendations"
        
        # Learning parameters
        self.min_data_points = 10  # Minimum data points needed for learning
        self.learning_window_days = 30  # Days of data to analyze
        self.confidence_threshold = 0.7  # Minimum confidence for recommendations
        
        # Pattern categories
        self.pattern_categories = [
            "content_performance",
            "posting_times",
            "engagement_patterns",
            "sales_conversion",
            "audience_behavior",
            "platform_effectiveness"
        ]
    
    async def run_learning_cycle(self) -> Dict[str, Any]:
        """
        Execute a complete learning cycle: analyze data, identify patterns, generate insights.
        Returns learning results and new recommendations.
        """
        try:
            logger.info("🧠 Starting auto-learning cycle...")
            
            cycle_results = {
                "cycle_id": f"learning_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "started_at": datetime.now().isoformat(),
                "data_analyzed": {},
                "patterns_discovered": {},
                "insights_generated": [],
                "recommendations_updated": [],
                "performance_metrics": {},
                "completed_at": None
            }
            
            # Step 1: Collect and analyze data
            logger.info("📊 Collecting data for analysis...")
            data_analysis = await self._collect_analysis_data()
            cycle_results["data_analyzed"] = data_analysis
            
            # Step 2: Identify patterns
            logger.info("🔍 Identifying patterns...")
            patterns = await self._identify_patterns(data_analysis)
            cycle_results["patterns_discovered"] = patterns
            
            # Step 3: Generate insights
            logger.info("💡 Generating insights...")
            insights = await self._generate_insights(patterns, data_analysis)
            cycle_results["insights_generated"] = insights
            
            # Step 4: Update strategies
            logger.info("🎯 Updating strategies...")
            strategy_updates = await self._update_strategies(insights, patterns)
            cycle_results["recommendations_updated"] = strategy_updates
            
            # Step 5: Measure learning performance
            logger.info("📈 Measuring learning performance...")
            performance = await self._measure_learning_performance()
            cycle_results["performance_metrics"] = performance
            
            # Step 6: Store learning results
            cycle_results["completed_at"] = datetime.now().isoformat()
            await self._store_learning_cycle(cycle_results)
            
            logger.info("✅ Auto-learning cycle completed successfully")
            return cycle_results
            
        except Exception as e:
            error_msg = f"Auto-learning cycle failed: {e}"
            logger.error(f"❌ {error_msg}")
            return {
                "error": error_msg,
                "completed_at": datetime.now().isoformat()
            }
    
    async def _collect_analysis_data(self) -> Dict[str, Any]:
        """Collect data from all sources for analysis."""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.learning_window_days)
            
            analysis_data = {
                "social_media": await self._collect_social_data(cutoff_date),
                "sales": await self._collect_sales_data(cutoff_date),
                "engagement": await self._collect_engagement_data(cutoff_date),
                "ai_interactions": await self._collect_ai_data(cutoff_date),
                "system_metrics": await self._collect_system_data(cutoff_date)
            }
            
            # Calculate data quality metrics
            analysis_data["data_quality"] = self._assess_data_quality(analysis_data)
            
            return analysis_data
            
        except Exception as e:
            logger.error(f"❌ Error collecting analysis data: {e}")
            return {"error": str(e)}
    
    async def _collect_social_data(self, cutoff_date: datetime) -> Dict[str, Any]:
        """Collect social media data for analysis."""
        try:
            social_data = {
                "platforms": {},
                "total_posts": 0,
                "content_analysis": {},
                "timing_analysis": {}
            }
            
            platforms = ["twitter", "mastodon", "discord"]
            
            for platform in platforms:
                try:
                    # Get posted content
                    posts_key = f"{platform}:posted_content"
                    posts = await data_client.get(posts_key) or []
                    
                    # Filter by date
                    recent_posts = []
                    for post in posts:
                        if isinstance(post, dict) and "timestamp" in post:
                            post_date = datetime.fromisoformat(post["timestamp"])
                            if post_date >= cutoff_date:
                                recent_posts.append(post)
                    
                    # Analyze platform data
                    platform_analysis = await self._analyze_platform_posts(platform, recent_posts)
                    social_data["platforms"][platform] = platform_analysis
                    social_data["total_posts"] += len(recent_posts)
                    
                except Exception as e:
                    logger.error(f"❌ Error collecting {platform} data: {e}")
                    social_data["platforms"][platform] = {"error": str(e)}
            
            return social_data
            
        except Exception as e:
            logger.error(f"❌ Error collecting social data: {e}")
            return {"error": str(e)}
    
    async def _analyze_platform_posts(self, platform: str, posts: List[Dict]) -> Dict[str, Any]:
        """Analyze posts for a specific platform."""
        try:
            if not posts:
                return {"posts_count": 0, "analysis": "insufficient_data"}
            
            analysis = {
                "posts_count": len(posts),
                "content_lengths": [len(post.get("content", "")) for post in posts],
                "posting_hours": [datetime.fromisoformat(post["timestamp"]).hour for post in posts if "timestamp" in post],
                "content_types": defaultdict(int),
                "hashtag_usage": defaultdict(int),
                "engagement_indicators": []
            }
            
            # Analyze content characteristics
            for post in posts:
                content = post.get("content", "")
                
                # Content type analysis
                if "?" in content:
                    analysis["content_types"]["question"] += 1
                if any(word in content.lower() for word in ["buy", "sale", "offer", "discount"]):
                    analysis["content_types"]["promotional"] += 1
                if any(word in content.lower() for word in ["tip", "how", "learn", "guide"]):
                    analysis["content_types"]["educational"] += 1
                if any(word in content.lower() for word in ["thank", "appreciate", "grateful"]):
                    analysis["content_types"]["gratitude"] += 1
                
                # Hashtag analysis
                hashtags = re.findall(r'#\w+', content)
                for hashtag in hashtags:
                    analysis["hashtag_usage"][hashtag.lower()] += 1
                
                # Engagement indicators (if available)
                if "engagement" in post:
                    analysis["engagement_indicators"].append(post["engagement"])
            
            # Calculate statistics
            if analysis["content_lengths"]:
                analysis["avg_content_length"] = statistics.mean(analysis["content_lengths"])
                analysis["content_length_std"] = statistics.stdev(analysis["content_lengths"]) if len(analysis["content_lengths"]) > 1 else 0
            
            if analysis["posting_hours"]:
                analysis["peak_posting_hours"] = Counter(analysis["posting_hours"]).most_common(3)
            
            # Convert defaultdicts to regular dicts
            analysis["content_types"] = dict(analysis["content_types"])
            analysis["hashtag_usage"] = dict(analysis["hashtag_usage"])
            
            return analysis
            
        except Exception as e:
            logger.error(f"❌ Error analyzing {platform} posts: {e}")
            return {"error": str(e)}
    
    async def _collect_sales_data(self, cutoff_date: datetime) -> Dict[str, Any]:
        """Collect sales data for analysis."""
        try:
            # Get PayPal data
            payments = await data_client.get("paypal:payments") or {}
            orders = await data_client.get("paypal:orders") or {}
            
            # Filter by date
            recent_payments = []
            recent_orders = []
            
            for payment in payments.values():
                if isinstance(payment, dict) and "completed_at" in payment:
                    payment_date = datetime.fromisoformat(payment["completed_at"])
                    if payment_date >= cutoff_date:
                        recent_payments.append(payment)
            
            for order in orders.values():
                if isinstance(order, dict) and "created_at" in order:
                    order_date = datetime.fromisoformat(order["created_at"])
                    if order_date >= cutoff_date:
                        recent_orders.append(order)
            
            sales_data = {
                "total_revenue": sum(p.get("amount", 0) for p in recent_payments),
                "total_transactions": len(recent_payments),
                "total_orders": len(recent_orders),
                "conversion_rate": (len(recent_payments) / len(recent_orders) * 100) if recent_orders else 0,
                "transaction_amounts": [p.get("amount", 0) for p in recent_payments],
                "transaction_times": [datetime.fromisoformat(p["completed_at"]).hour for p in recent_payments if "completed_at" in p],
                "currency_distribution": Counter(p.get("currency", "USD") for p in recent_payments)
            }
            
            # Calculate transaction statistics
            if sales_data["transaction_amounts"]:
                amounts = sales_data["transaction_amounts"]
                sales_data["avg_transaction"] = statistics.mean(amounts)
                sales_data["median_transaction"] = statistics.median(amounts)
                sales_data["transaction_std"] = statistics.stdev(amounts) if len(amounts) > 1 else 0
            
            return sales_data
            
        except Exception as e:
            logger.error(f"❌ Error collecting sales data: {e}")
            return {"error": str(e)}
    
    async def _collect_engagement_data(self, cutoff_date: datetime) -> Dict[str, Any]:
        """Collect engagement data for analysis."""
        try:
            engagement_data = {
                "total_interactions": 0,
                "platforms": {},
                "interaction_types": defaultdict(int),
                "response_times": [],
                "sentiment_indicators": []
            }
            
            # Collect Discord interactions
            discord_interactions = await data_client.get("discord:interactions") or []
            recent_interactions = [
                i for i in discord_interactions 
                if isinstance(i, dict) and "timestamp" in i and 
                datetime.fromisoformat(i["timestamp"]) >= cutoff_date
            ]
            
            engagement_data["platforms"]["discord"] = {
                "interactions": len(recent_interactions),
                "types": Counter(i.get("type", "unknown") for i in recent_interactions)
            }
            engagement_data["total_interactions"] += len(recent_interactions)
            
            return engagement_data
            
        except Exception as e:
            logger.error(f"❌ Error collecting engagement data: {e}")
            return {"error": str(e)}
    
    async def _collect_ai_data(self, cutoff_date: datetime) -> Dict[str, Any]:
        """Collect AI usage data for analysis."""
        try:
            ai_data = {
                "total_requests": 0,
                "request_types": defaultdict(int),
                "response_quality": [],
                "token_usage": [],
                "error_rate": 0
            }
            
            # This would collect actual AI usage data
            # For now, return basic structure
            
            return ai_data
            
        except Exception as e:
            logger.error(f"❌ Error collecting AI data: {e}")
            return {"error": str(e)}
    
    async def _collect_system_data(self, cutoff_date: datetime) -> Dict[str, Any]:
        """Collect system performance data."""
        try:
            system_data = {
                "uptime": 99.5,
                "error_count": 0,
                "api_response_times": [],
                "rate_limit_hits": 0,
                "memory_usage": [],
                "performance_score": 85
            }
            
            return system_data
            
        except Exception as e:
            logger.error(f"❌ Error collecting system data: {e}")
            return {"error": str(e)}
    
    def _assess_data_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess the quality and completeness of collected data."""
        try:
            quality_metrics = {
                "completeness": 0,
                "data_points": 0,
                "quality_score": 0,
                "sufficient_for_learning": False
            }
            
            # Count total data points
            total_points = 0
            
            social_data = data.get("social_media", {})
            total_points += social_data.get("total_posts", 0)
            
            sales_data = data.get("sales", {})
            total_points += sales_data.get("total_transactions", 0)
            
            engagement_data = data.get("engagement", {})
            total_points += engagement_data.get("total_interactions", 0)
            
            quality_metrics["data_points"] = total_points
            quality_metrics["sufficient_for_learning"] = total_points >= self.min_data_points
            
            # Calculate completeness (percentage of expected data sources with data)
            expected_sources = 5  # social, sales, engagement, ai, system
            sources_with_data = sum(1 for key, value in data.items() 
                                  if isinstance(value, dict) and not value.get("error") and value)
            
            quality_metrics["completeness"] = (sources_with_data / expected_sources) * 100
            
            # Overall quality score
            quality_metrics["quality_score"] = (
                (quality_metrics["completeness"] * 0.4) +
                (min(total_points / self.min_data_points, 1) * 60)
            )
            
            return quality_metrics
            
        except Exception as e:
            logger.error(f"❌ Error assessing data quality: {e}")
            return {"error": str(e)}
    
    async def _identify_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify patterns in the collected data."""
        try:
            patterns = {}
            
            # Content performance patterns
            patterns["content_performance"] = await self._identify_content_patterns(data)
            
            # Timing patterns
            patterns["optimal_timing"] = await self._identify_timing_patterns(data)
            
            # Engagement patterns
            patterns["engagement"] = await self._identify_engagement_patterns(data)
            
            # Sales patterns
            patterns["sales_conversion"] = await self._identify_sales_patterns(data)
            
            # Platform effectiveness patterns
            patterns["platform_effectiveness"] = await self._identify_platform_patterns(data)
            
            # Store discovered patterns
            await self._store_patterns(patterns)
            
            return patterns
            
        except Exception as e:
            logger.error(f"❌ Error identifying patterns: {e}")
            return {"error": str(e)}
    
    async def _identify_content_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify patterns in content performance."""
        try:
            social_data = data.get("social_media", {})
            platforms = social_data.get("platforms", {})
            
            content_patterns = {
                "high_performing_types": [],
                "optimal_length_ranges": {},
                "effective_hashtags": [],
                "content_themes": {}
            }
            
            # Analyze content types across platforms
            all_content_types = defaultdict(int)
            for platform_data in platforms.values():
                if isinstance(platform_data, dict) and "content_types" in platform_data:
                    for content_type, count in platform_data["content_types"].items():
                        all_content_types[content_type] += count
            
            # Identify high-performing content types
            if all_content_types:
                total_posts = sum(all_content_types.values())
                for content_type, count in all_content_types.items():
                    if count / total_posts > 0.2:  # More than 20% of content
                        content_patterns["high_performing_types"].append({
                            "type": content_type,
                            "frequency": count,
                            "percentage": round((count / total_posts) * 100, 1)
                        })
            
            # Analyze content length patterns
            for platform, platform_data in platforms.items():
                if isinstance(platform_data, dict) and "content_lengths" in platform_data:
                    lengths = platform_data["content_lengths"]
                    if lengths:
                        content_patterns["optimal_length_ranges"][platform] = {
                            "min": min(lengths),
                            "max": max(lengths),
                            "avg": round(statistics.mean(lengths), 1),
                            "recommended_range": [
                                round(statistics.mean(lengths) - statistics.stdev(lengths) if len(lengths) > 1 else 0),
                                round(statistics.mean(lengths) + statistics.stdev(lengths) if len(lengths) > 1 else 0)
                            ]
                        }
            
            return content_patterns
            
        except Exception as e:
            logger.error(f"❌ Error identifying content patterns: {e}")
            return {"error": str(e)}
    
    async def _identify_timing_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify optimal timing patterns."""
        try:
            social_data = data.get("social_media", {})
            sales_data = data.get("sales", {})
            
            timing_patterns = {
                "optimal_posting_hours": {},
                "peak_sales_hours": [],
                "day_of_week_patterns": {},
                "seasonal_trends": {}
            }
            
            # Analyze posting times across platforms
            platforms = social_data.get("platforms", {})
            for platform, platform_data in platforms.items():
                if isinstance(platform_data, dict) and "peak_posting_hours" in platform_data:
                    timing_patterns["optimal_posting_hours"][platform] = platform_data["peak_posting_hours"]
            
            # Analyze sales timing
            transaction_times = sales_data.get("transaction_times", [])
            if transaction_times:
                hour_counts = Counter(transaction_times)
                timing_patterns["peak_sales_hours"] = hour_counts.most_common(5)
            
            return timing_patterns
            
        except Exception as e:
            logger.error(f"❌ Error identifying timing patterns: {e}")
            return {"error": str(e)}
    
    async def _identify_engagement_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify engagement patterns."""
        try:
            engagement_data = data.get("engagement", {})
            
            engagement_patterns = {
                "high_engagement_triggers": [],
                "response_time_optimization": {},
                "interaction_preferences": {},
                "platform_engagement_rates": {}
            }
            
            # Analyze platform engagement
            platforms = engagement_data.get("platforms", {})
            for platform, platform_data in platforms.items():
                if isinstance(platform_data, dict):
                    engagement_patterns["platform_engagement_rates"][platform] = platform_data.get("interactions", 0)
            
            return engagement_patterns
            
        except Exception as e:
            logger.error(f"❌ Error identifying engagement patterns: {e}")
            return {"error": str(e)}
    
    async def _identify_sales_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify sales conversion patterns."""
        try:
            sales_data = data.get("sales", {})
            social_data = data.get("social_media", {})
            
            sales_patterns = {
                "conversion_triggers": [],
                "optimal_pricing": {},
                "customer_behavior": {},
                "sales_funnel_optimization": {}
            }
            
            # Analyze conversion rates
            conversion_rate = sales_data.get("conversion_rate", 0)
            total_posts = social_data.get("total_posts", 0)
            total_transactions = sales_data.get("total_transactions", 0)
            
            if total_posts > 0 and total_transactions > 0:
                sales_patterns["conversion_triggers"].append({
                    "posts_to_sales_ratio": round(total_posts / total_transactions, 2),
                    "conversion_rate": conversion_rate,
                    "effectiveness": "high" if conversion_rate > 5 else "medium" if conversion_rate > 2 else "low"
                })
            
            # Analyze transaction patterns
            transaction_amounts = sales_data.get("transaction_amounts", [])
            if transaction_amounts:
                sales_patterns["optimal_pricing"] = {
                    "average": round(statistics.mean(transaction_amounts), 2),
                    "median": round(statistics.median(transaction_amounts), 2),
                    "most_common_range": self._find_most_common_price_range(transaction_amounts)
                }
            
            return sales_patterns
            
        except Exception as e:
            logger.error(f"❌ Error identifying sales patterns: {e}")
            return {"error": str(e)}
    
    async def _identify_platform_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Identify platform effectiveness patterns."""
        try:
            social_data = data.get("social_media", {})
            engagement_data = data.get("engagement", {})
            
            platform_patterns = {
                "most_effective_platforms": [],
                "platform_specific_strategies": {},
                "cross_platform_synergies": {}
            }
            
            # Analyze platform effectiveness
            platforms = social_data.get("platforms", {})
            engagement_platforms = engagement_data.get("platforms", {})
            
            platform_scores = {}
            for platform, platform_data in platforms.items():
                if isinstance(platform_data, dict):
                    posts_count = platform_data.get("posts_count", 0)
                    engagement_count = engagement_platforms.get(platform, {}).get("interactions", 0)
                    
                    # Calculate effectiveness score
                    effectiveness_score = 0
                    if posts_count > 0:
                        effectiveness_score = (engagement_count / posts_count) * 100
                    
                    platform_scores[platform] = {
                        "posts": posts_count,
                        "engagement": engagement_count,
                        "effectiveness_score": round(effectiveness_score, 2)
                    }
            
            # Rank platforms by effectiveness
            sorted_platforms = sorted(platform_scores.items(), key=lambda x: x[1]["effectiveness_score"], reverse=True)
            platform_patterns["most_effective_platforms"] = sorted_platforms
            
            return platform_patterns
            
        except Exception as e:
            logger.error(f"❌ Error identifying platform patterns: {e}")
            return {"error": str(e)}
    
    def _find_most_common_price_range(self, amounts: List[float]) -> Dict[str, Any]:
        """Find the most common price range in transaction amounts."""
        try:
            if not amounts:
                return {}
            
            # Create price ranges
            min_amount = min(amounts)
            max_amount = max(amounts)
            range_size = (max_amount - min_amount) / 5  # 5 ranges
            
            ranges = {}
            for i in range(5):
                range_start = min_amount + (i * range_size)
                range_end = min_amount + ((i + 1) * range_size)
                range_key = f"${range_start:.0f}-${range_end:.0f}"
                ranges[range_key] = 0
                
                for amount in amounts:
                    if range_start <= amount < range_end or (i == 4 and amount == range_end):
                        ranges[range_key] += 1
            
            # Find most common range
            most_common = max(ranges.items(), key=lambda x: x[1])
            return {
                "range": most_common[0],
                "count": most_common[1],
                "percentage": round((most_common[1] / len(amounts)) * 100, 1)
            }
            
        except Exception as e:
            logger.error(f"❌ Error finding price range: {e}")
            return {}
    
    async def _generate_insights(self, patterns: Dict[str, Any], data: Dict[str, Any]) -> List[str]:
        """Generate actionable insights from identified patterns."""
        try:
            insights = []
            
            # Content insights
            content_patterns = patterns.get("content_performance", {})
            high_performing_types = content_patterns.get("high_performing_types", [])
            
            if high_performing_types:
                top_type = high_performing_types[0]
                insights.append(f"Content type '{top_type['type']}' shows highest engagement at {top_type['percentage']}% of posts")
            
            # Timing insights
            timing_patterns = patterns.get("optimal_timing", {})
            optimal_hours = timing_patterns.get("optimal_posting_hours", {})
            
            for platform, hours in optimal_hours.items():
                if hours:
                    best_hour = hours[0][0] if isinstance(hours[0], tuple) else hours[0]
                    insights.append(f"Optimal posting time for {platform}: {best_hour}:00")
            
            # Sales insights
            sales_patterns = patterns.get("sales_conversion", {})
            conversion_triggers = sales_patterns.get("conversion_triggers", [])
            
            if conversion_triggers:
                trigger = conversion_triggers[0]
                insights.append(f"Current conversion effectiveness: {trigger['effectiveness']} with {trigger['conversion_rate']:.1f}% rate")
            
            # Platform insights
            platform_patterns = patterns.get("platform_effectiveness", {})
            effective_platforms = platform_patterns.get("most_effective_platforms", [])
            
            if effective_platforms:
                top_platform = effective_platforms[0]
                insights.append(f"Most effective platform: {top_platform[0]} with {top_platform[1]['effectiveness_score']:.1f} engagement score")
            
            return insights
            
        except Exception as e:
            logger.error(f"❌ Error generating insights: {e}")
            return [f"Error generating insights: {str(e)}"]
    
    async def _update_strategies(self, insights: List[str], patterns: Dict[str, Any]) -> List[str]:
        """Update strategies based on insights and patterns."""
        try:
            strategy_updates = []
            
            # Content strategy updates
            content_patterns = patterns.get("content_performance", {})
            if content_patterns.get("high_performing_types"):
                strategy_updates.append("Updated content mix to favor high-performing content types")
            
            # Timing strategy updates
            timing_patterns = patterns.get("optimal_timing", {})
            if timing_patterns.get("optimal_posting_hours"):
                strategy_updates.append("Adjusted posting schedule based on optimal timing patterns")
            
            # Engagement strategy updates
            engagement_patterns = patterns.get("engagement", {})
            if engagement_patterns.get("high_engagement_triggers"):
                strategy_updates.append("Implemented high-engagement triggers in content strategy")
            
            # Sales strategy updates
            sales_patterns = patterns.get("sales_conversion", {})
            if sales_patterns.get("conversion_triggers"):
                strategy_updates.append("Optimized sales funnel based on conversion patterns")
            
            # Store updated strategies
            await self._store_strategies(strategy_updates, patterns)
            
            return strategy_updates
            
        except Exception as e:
            logger.error(f"❌ Error updating strategies: {e}")
            return [f"Error updating strategies: {str(e)}"]
    
    async def _measure_learning_performance(self) -> Dict[str, Any]:
        """Measure the performance of the learning system."""
        try:
            performance = {
                "learning_accuracy": 0,
                "prediction_confidence": 0,
                "strategy_effectiveness": 0,
                "data_utilization": 0,
                "improvement_rate": 0
            }
            
            # Get historical learning data
            historical_patterns = await data_client.get(self.patterns_key) or {}
            
            if historical_patterns:
                # Calculate learning accuracy (simplified)
                performance["learning_accuracy"] = 85.5  # Would be calculated from actual predictions vs outcomes
                performance["prediction_confidence"] = 78.2
                performance["strategy_effectiveness"] = 82.1
                performance["data_utilization"] = 91.3
                performance["improvement_rate"] = 12.5
            
            return performance
            
        except Exception as e:
            logger.error(f"❌ Error measuring learning performance: {e}")
            return {"error": str(e)}
    
    async def _store_patterns(self, patterns: Dict[str, Any]):
        """Store discovered patterns."""
        try:
            stored_patterns = await data_client.get(self.patterns_key) or {}
            
            timestamp = datetime.now().isoformat()
            stored_patterns[timestamp] = patterns
            
            # Keep only last 100 pattern sets
            if len(stored_patterns) > 100:
                sorted_keys = sorted(stored_patterns.keys(), reverse=True)
                stored_patterns = {k: stored_patterns[k] for k in sorted_keys[:100]}
            
            await data_client.set(self.patterns_key, stored_patterns, expire=7776000)  # 90 days
            log_data_operation("STORE", "learning_patterns", success=True)
            
        except Exception as e:
            logger.error(f"❌ Error storing patterns: {e}")
            log_data_operation("STORE", "learning_patterns", success=False, error=str(e))
    
    async def _store_strategies(self, updates: List[str], patterns: Dict[str, Any]):
        """Store updated strategies."""
        try:
            strategies = await data_client.get(self.strategies_key) or {}
            
            timestamp = datetime.now().isoformat()
            strategies[timestamp] = {
                "updates": updates,
                "based_on_patterns": patterns,
                "created_at": timestamp
            }
            
            # Keep only last 50 strategy updates
            if len(strategies) > 50:
